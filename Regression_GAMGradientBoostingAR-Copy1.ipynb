{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5b0b29-f218-42be-ad49-250258fa3f5c",
   "metadata": {},
   "source": [
    "On rajoute les données meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ade40bd-d76a-496b-a920-9cf77202eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46710d7f-b791-41d9-8e44-3e5b41f8d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_debit = pd.read_csv(\"../Data/Base/Stations_Debit.csv\")\n",
    "liste_stations_debit = list(stations_debit[\"Code station\"])\n",
    "\n",
    "mesures_train_X = pd.read_csv(\"../Data/Base/Mesures_Train_X.csv\")\n",
    "mesures_train_X[\"Date\"] = pd.to_datetime(mesures_train_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_train_Y = pd.read_csv(\"../Data/Base/Mesures_Train_Y.csv\")\n",
    "mesures_train_Y[\"Date\"] = pd.to_datetime(mesures_train_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "train_split_X = pd.read_csv(\"../Data/Base/Index_CV_X.csv\")\n",
    "train_split_Y = pd.read_csv(\"../Data/Base/Index_CV_Y.csv\")\n",
    "\n",
    "mesures_test_X = pd.read_csv(\"../Data/Base/Mesures_Test_X.csv\")\n",
    "mesures_test_X[\"Date\"] = pd.to_datetime(mesures_test_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_test_Y = pd.read_csv(\"../Data/Base/Mesures_Test_Y.csv\")\n",
    "mesures_test_Y[\"Date\"] = pd.to_datetime(mesures_test_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8752eee-0b85-4a1f-9781-d7b6d3d9e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo = pd.read_csv(\"../Data/Base/Stations_Meteo.csv\")\n",
    "liste_stations_meteo = list(stations_meteo[\"ID\"].apply(lambda x: str(x).rjust(5, \"0\")))\n",
    "colonnes_meteo = [\"Pression\", \"Vent_Nord\", \"Vent_Est\", \"Vitesse_vent\", \"Temperature\", \"Humidite\", \"Precipitations\"]\n",
    "tempo = []\n",
    "for code in liste_stations_meteo:\n",
    "    tempo += [tp + \"_\" + code for tp in colonnes_meteo]\n",
    "colonnes_meteo_stations = tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9732a271-7af8-4f7c-8840-2dcbbb4bb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Eval_fcts.py\n",
    "%run Standardize_fcts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c90c5f1-b157-46a8-b3cc-8034f8195fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Regression_GAM_Gamma.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b81e78-551c-47ca-9e0b-ebd3d94d52ea",
   "metadata": {},
   "source": [
    "# Val croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb59d54-eebd-4184-8065-200bd98d1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_knots = 10\n",
    "lr_grid = [1, 3, 10, 30, 100, 300, 1000]\n",
    "max_lag = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd14f45-0ebd-45df-83df-cd71c897e2a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 8/8 [9:28:26<00:00, 4263.29s/it]\n",
      " 38%|██████████████▋                        | 3/8 [2:41:58<4:29:04, 3228.86s/it]"
     ]
    }
   ],
   "source": [
    "for curr_lr in lr_grid:\n",
    "    for curr_lag in tqdm(range(0,max_lag+1)):\n",
    "        cv_scores_RMSE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_MAE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_R2_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "        cv_scores_RMSE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_MAE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_R2 = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "        for curr_split in range(9):\n",
    "\n",
    "            # Donnees\n",
    "            curr_train_X = mesures_train_X[train_split_X[\"Train_\" + str(curr_split)]]\n",
    "            curr_train_Y = mesures_train_Y[train_split_Y[\"Train_\" + str(curr_split)]]\n",
    "            curr_test_X = mesures_train_X[train_split_X[\"Test_\" + str(curr_split)]]\n",
    "            curr_test_Y = mesures_train_Y[train_split_Y[\"Test_\" + str(curr_split)]]\n",
    "            liste_dates = curr_test_Y[\"Date\"]\n",
    "\n",
    "            curr_train_X_mean = curr_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "            for code in liste_stations_debit:\n",
    "                curr_train_X_mean[code] = 0\n",
    "            curr_train_X_std = curr_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "            curr_train_X_standard = fct_Standardize(curr_train_X, curr_train_X_mean,\n",
    "                                                    curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "            curr_train_X_standard[\"Date\"] = curr_train_X[\"Date\"]\n",
    "            #GAM\n",
    "            spline_fit = fct_Regression_SplineGamma_fit(curr_train_X_standard, liste_stations_debit, n_knots)\n",
    "            curr_train_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit, \n",
    "                                                                               curr_train_X_standard, \n",
    "                                                                               liste_stations_debit)\n",
    "            curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, \n",
    "                                                          curr_train_X_standard[colonnes_meteo_stations]],\n",
    "                                                         axis = 1)\n",
    "            \n",
    "             # Ajout AR\n",
    "            colonnes_reg = colonnes_meteo_stations\n",
    "            for i in range(curr_lag+1):\n",
    "                variable = curr_train_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "                variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "                curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, variable], axis = 1)\n",
    "                colonnes_reg = colonnes_reg + [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "            curr_train_X_standard_residus = curr_train_X_standard_residus[(7+curr_lag):]\n",
    "            \n",
    "            curr_test_X_standard =fct_Standardize(curr_test_X, curr_train_X_mean,\n",
    "                                                  curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "            curr_test_X_standard[\"Date\"] = curr_test_X[\"Date\"].values\n",
    "            curr_test_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit,\n",
    "                                                                              curr_test_X_standard, \n",
    "                                                                              liste_stations_debit)\n",
    "            curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, \n",
    "                                                      curr_test_X_standard[colonnes_meteo_stations]],\n",
    "                                                     axis = 1)\n",
    "            # Ajout variables AR\n",
    "            for i in range(curr_lag+1):\n",
    "                variable = curr_test_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "                variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "                curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, variable], axis = 1)\n",
    "            curr_test_X_standard_residus = curr_test_X_standard_residus[(7+curr_lag):]\n",
    "            \n",
    "            curr_test_Y_standard = fct_Standardize(curr_test_Y, curr_train_X_mean,\n",
    "                                                    curr_train_X_std, liste_stations_debit)\n",
    "            curr_test_Y_standard[\"Date\"] = curr_test_Y[\"Date\"]\n",
    "\n",
    "            curr_RMSE_standard = []\n",
    "            curr_MAE_standard = []\n",
    "            curr_R2_standard = []\n",
    "            curr_RMSE = []\n",
    "            curr_MAE = []\n",
    "            curr_R2 = []\n",
    "            \n",
    "            for code in liste_stations_debit:\n",
    "\n",
    "                # Entrainement\n",
    "                model = GradientBoostingRegressor(learning_rate = curr_lr/1000)\n",
    "                model.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "                      y = curr_train_X_standard_residus[code])\n",
    "\n",
    "                # Predictions Standard\n",
    "                predictions_Y_standard_residus = model.predict(curr_test_X_standard_residus[colonnes_reg])\n",
    "                predictions_Y_standard_residus = pd.DataFrame(predictions_Y_standard_residus, columns=[code])\n",
    "                predictions_Y_standard_residus[\"Date\"] = curr_test_X_standard_residus[\"Date\"].values\n",
    "                # Filtre sur les dates\n",
    "                resultat = pd.DataFrame()\n",
    "                for curr_date in liste_dates:\n",
    "                    resultat = pd.concat([resultat, predictions_Y_standard_residus[predictions_Y_standard_residus[\"Date\"] == curr_date]])\n",
    "                resultat = resultat.sort_values(by = \"Date\")\n",
    "                predictions_Y_standard_residus = resultat.copy()\n",
    "                # On rajoute la compo saisonnalité\n",
    "                predictions_Y_standard_saisonnalite = fct_Regression_SplineGamma_predict(spline_fit, liste_dates, [code])\n",
    "                predictions_Y_standard = predictions_Y_standard_residus[[\"Date\"]].copy()\n",
    "                predictions_Y_standard[code] = predictions_Y_standard_residus[code].values + predictions_Y_standard_saisonnalite[code].values\n",
    "\n",
    "                # Scores standards\n",
    "                curr_RMSE_standard.append(fct_RMSE(curr_test_Y_standard, predictions_Y_standard, [code])[\"RMSE\"][0])\n",
    "                curr_MAE_standard.append(fct_MAE(curr_test_Y_standard, predictions_Y_standard, [code])[\"MAE\"][0])\n",
    "                curr_R2_standard.append(fct_R2(curr_test_Y_standard, predictions_Y_standard, [code])[\"R2\"][0])\n",
    "                # Score\n",
    "                predictions_Y = fct_StandardizeInverse(predictions_Y_standard, \n",
    "                                                       curr_train_X_mean, curr_train_X_std, \n",
    "                                                       [code])\n",
    "                predictions_Y[\"Date\"] = predictions_Y_standard[\"Date\"].values\n",
    "                curr_RMSE.append(fct_RMSE(curr_test_Y, predictions_Y, [code])[\"RMSE\"][0])\n",
    "                curr_MAE.append(fct_MAE(curr_test_Y, predictions_Y, [code])[\"MAE\"][0])\n",
    "                curr_R2.append(fct_R2(curr_test_Y, predictions_Y, [code])[\"R2\"][0])\n",
    "\n",
    "            # On rassemble\n",
    "            cv_scores_RMSE_standard[\"Split_\" + str(curr_split)] = curr_RMSE_standard\n",
    "            cv_scores_MAE_standard[\"Split_\" + str(curr_split)] = curr_MAE_standard\n",
    "            cv_scores_R2_standard[\"Split_\" + str(curr_split)] = curr_R2_standard\n",
    "\n",
    "            cv_scores_RMSE[\"Split_\" + str(curr_split)] = curr_RMSE\n",
    "            cv_scores_MAE[\"Split_\" + str(curr_split)] = curr_MAE\n",
    "            cv_scores_R2[\"Split_\" + str(curr_split)] = curr_R2\n",
    "\n",
    "        # Calcul des scores moyens du split\n",
    "        cv_moyen_RMSE_standard = []\n",
    "        cv_moyen_MAE_standard = []\n",
    "        cv_moyen_R2_standard = []\n",
    "        cv_moyen_RMSE = []\n",
    "        cv_moyen_MAE = []\n",
    "        cv_moyen_R2 = []\n",
    "\n",
    "        for code in liste_stations_debit:\n",
    "            score_RMSE = np.mean(cv_scores_RMSE_standard[cv_scores_RMSE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_RMSE_standard.append(score_RMSE)\n",
    "            score_MAE = np.mean(cv_scores_MAE_standard[cv_scores_MAE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_MAE_standard.append(score_MAE)\n",
    "            score_R2 = np.mean(cv_scores_R2_standard[cv_scores_R2_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_R2_standard.append(score_R2)\n",
    "            score_RMSE = np.mean(cv_scores_RMSE[cv_scores_RMSE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_RMSE.append(score_RMSE)\n",
    "            score_MAE = np.mean(cv_scores_MAE[cv_scores_MAE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_MAE.append(score_MAE)\n",
    "            score_R2 = np.mean(cv_scores_R2[cv_scores_R2[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_R2.append(score_R2)\n",
    "\n",
    "        cv_scores_RMSE_standard[\"Moyenne\"] = cv_moyen_RMSE_standard\n",
    "        cv_scores_RMSE_standard.to_csv(\"../Data/GAMGradientBoostingAR/CV_RMSE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_MAE_standard[\"Moyenne\"] = cv_moyen_MAE_standard\n",
    "        cv_scores_MAE_standard.to_csv(\"../Data/GAMGradientBoostingAR/CV_MAE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_R2_standard[\"Moyenne\"] = cv_moyen_R2_standard\n",
    "        cv_scores_R2_standard.to_csv(\"../Data/GAMGradientBoostingAR/CV_R2_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_RMSE[\"Moyenne\"] = cv_moyen_RMSE\n",
    "        cv_scores_RMSE.to_csv(\"../Data/GAMGradientBoostingAR/CV_RMSE_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_MAE[\"Moyenne\"] = cv_moyen_MAE\n",
    "        cv_scores_MAE.to_csv(\"../Data/GAMGradientBoostingAR/CV_MAE_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_R2[\"Moyenne\"] = cv_moyen_R2\n",
    "        cv_scores_R2.to_csv(\"../Data/GAMGradientBoostingAR/CV_R2_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366761a-87df-4e2e-8b01-12a21cbe4ae4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "colonnes_meteo_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ad0e-3717-4b5f-b654-be16ba4dbd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819eda28-32e9-45df-96eb-3f544562eac8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cours_eau = 3\n",
    "cours_eau = list(np.unique(stations_debit[\"Cours eau\"]))\n",
    "cours_eau_cmap = cm.get_cmap(ListedColormap([\"red\", \"green\", \"blue\"]))\n",
    "cours_eau_couleur = pd.DataFrame({\"Cours eau\": cours_eau, \"Index\": range(n_cours_eau), \"Couleur\": [\"red\", \"green\", \"blue\"]})\n",
    "cours_eau_couleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f8fff-6421-4623-8bf1-33c74b8ca232",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_RMSE_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb63cdd-51fd-4507-bacf-9010be71a177",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_MAE_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342c8f-b28b-48ae-9f57-34d27b922168",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_R2_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f6b7b-6157-44ca-9696-fe3824c28e8b",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00efb65-f12b-4490-b3c5-cb53804ce78e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_RMSE_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    " + \".csv\")    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3f190-39d9-42a0-b1fb-eb56073dac81",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_MAE_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809a4cb-c188-490e-8dbb-d9962dfb95fb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GradientBoosting_Meteo/CV_R2_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1ecee-9eb8-4953-ad7a-efbc07a9fd34",
   "metadata": {},
   "source": [
    "## Moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c518f-5a58-4ab0-bd86-ca4316c70175",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_RMSE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715d107-be51-467a-8278-4312869b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_RMSE_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c4cd7-7da7-4017-bd54-14680a31c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_RMSE_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d4432-845d-4d2f-b5db-8b5e1ec93b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_MAE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e9a4d-90a9-4f3b-b462-e1ec594de7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_MAE_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988a053-0af1-4d04-9778-8a259f43b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_MAE_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f138cab-6305-4224-85cf-3900c2e1a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_R2_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88716ef-f4e0-4d7c-882f-419fe76067fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_R2_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221a677-c8b9-43e3-b507-354e4cd63d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMBoostAR/GAMBoostAR_R2_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb63b23-803c-4db2-b015-0e4ceac78a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_RMSE_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540949d9-0aee-434c-939a-c4af87b87546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_MAE_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6351422-08c1-4124-ae54-245087e26077",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMGradientBoostingAR/CV_R2_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b546cd-b011-40e7-bfda-b739c6531a21",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4df88e-e968-418d-bdae-3bfc80075d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6925f2-3461-45c6-9fce-75678cc218d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donnees\n",
    "mesures_train_X_mean = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "mesures_train_X_std = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "mesures_train_X_standard = fct_Standardize(mesures_train_X, \n",
    "                                           mesures_train_X_mean, mesures_train_X_std, \n",
    "                                           liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_train_X_standard[\"Date\"] = mesures_train_X[\"Date\"]\n",
    "\n",
    "mesures_test_X_standard =fct_Standardize(mesures_test_X, mesures_train_X_mean, mesures_train_X_std, \n",
    "                                         liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_test_X_standard[\"Date\"] = mesures_test_X[\"Date\"].values\n",
    "mesures_test_Y_standard = fct_Standardize(mesures_test_Y, mesures_train_X_mean, mesures_train_X_std, \n",
    "                                          liste_stations_debit)\n",
    "mesures_test_Y_standard[\"Date\"] = mesures_test_Y[\"Date\"]\n",
    "liste_dates = mesures_test_Y[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3aa27-10de-4d3e-8de6-9a5985d20530",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_RMSE_standard = []\n",
    "scores_MAE_standard = []\n",
    "scores_R2_standard = []\n",
    "scores_RMSE = []\n",
    "scores_MAE = []\n",
    "scores_R2 = []\n",
    "\n",
    "predictions_test_Y_standard = mesures_test_Y[[\"Date\"]]\n",
    "predictions_test_Y = mesures_test_Y[[\"Date\"]]\n",
    "\n",
    "for code in tqdm(liste_stations_debit):\n",
    "        \n",
    "        \n",
    "        # Entrainement\n",
    "        model = GradientBoostingRegressor(learning_rate = lr/1000)\n",
    "        model.fit(X = mesures_train_X_standard[colonnes_meteo_stations],\n",
    "                  y = mesures_train_X_standard[code])\n",
    "        \n",
    "        # Predictions Standard\n",
    "        predictions_Y_standard = model.predict(mesures_test_X_standard[colonnes_meteo_stations])\n",
    "        predictions_Y_standard = pd.DataFrame(predictions_Y_standard, columns=[code])\n",
    "        predictions_Y_standard[\"Date\"] = mesures_test_X_standard[\"Date\"].values\n",
    "        # Filtre sur les dates\n",
    "        resultat = pd.DataFrame()\n",
    "        for curr_date in liste_dates:\n",
    "            resultat = pd.concat([resultat, predictions_Y_standard[predictions_Y_standard[\"Date\"] == curr_date]])\n",
    "        resultat = resultat.sort_values(by = \"Date\")\n",
    "        predictions_Y_standard = resultat.copy()\n",
    "        predictions_test_Y_standard = predictions_test_Y_standard.merge(predictions_Y_standard)\n",
    "        \n",
    "        predictions_Y = fct_StandardizeInverse(predictions_Y_standard, \n",
    "                                                   mesures_train_X_mean, mesures_train_X_std, \n",
    "                                                   [code])\n",
    "        predictions_Y[\"Date\"] = predictions_Y_standard[\"Date\"].values\n",
    "        predictions_test_Y = predictions_test_Y.merge(predictions_Y)\n",
    "        \n",
    "        # Scores standards\n",
    "        scores_RMSE_standard.append(fct_RMSE(mesures_test_Y_standard, predictions_Y_standard, [code])[\"RMSE\"][0])\n",
    "        scores_MAE_standard.append(fct_MAE(mesures_test_Y_standard, predictions_Y_standard, [code])[\"MAE\"][0])\n",
    "        scores_R2_standard.append(fct_R2(mesures_test_Y_standard, predictions_Y_standard, [code])[\"R2\"][0])\n",
    "        # Score\n",
    "        scores_RMSE.append(fct_RMSE(mesures_test_Y, predictions_Y, [code])[\"RMSE\"][0])\n",
    "        scores_MAE.append(fct_MAE(mesures_test_Y, predictions_Y, [code])[\"MAE\"][0])\n",
    "        scores_R2.append(fct_R2(mesures_test_Y, predictions_Y, [code])[\"R2\"][0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382f33f-4913-4ea7-bbd0-b4020f4f61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_RMSE,\n",
    "                            \"MAE\": scores_MAE,\n",
    "                            \"R2\": scores_R2})\n",
    "test_scores.to_csv(\"../Data/GAMGradientBoostingAR/Test_scores.csv\")\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e650086-8f4d-459b-9b3b-e17e68ed6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_standard = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_RMSE,\n",
    "                            \"MAE\": scores_MAE,\n",
    "                            \"R2\": scores_R2})\n",
    "test_scores_standard.to_csv(\"../Data/GAMGradientBoostingAR/Test_scores_standard.csv\")\n",
    "test_scores_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d21b-9d11-49d1-9b90-a14fc12e0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_standard[[\"RMSE\", \"MAE\", \"R2\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415553c-cdaf-4e30-84a3-e94c1b0ae4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = len(liste_stations_debit)\n",
    "fig, axs = plt.subplots(n_stations, 1, figsize = (15,30), sharex=True)\n",
    "for i in range(n_stations):\n",
    "    code = liste_stations_debit[i]\n",
    "    axs[i].plot(mesures_test_Y_standard[code], label = \"True\")\n",
    "    axs[i].plot(predictions_test_Y_standard[code], label = \"Predictions\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780d10b-c50d-4704-8573-8f505095a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = len(liste_stations_debit)\n",
    "fig, axs = plt.subplots(n_stations, 1, figsize = (15,30), sharex=True)\n",
    "for i in range(n_stations):\n",
    "    code = liste_stations_debit[i]\n",
    "    axs[i].plot(mesures_test_Y[code], label = \"True\")\n",
    "    axs[i].plot(predictions_test_Y[code], label = \"Predictions\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab74395-3ce5-42bd-bdae-87f1d887c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0])[0:20] + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx][0:20], align=\"center\")\n",
    "plt.yticks(pos, np.array(colonnes_meteo_stations)[sorted_idx][0:20])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "result = permutation_importance(\n",
    "    model, mesures_test_X_standard[colonnes_meteo_stations], mesures_test_X_standard[code], n_repeats=10, random_state=42, n_jobs=20\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx][0:20].T,\n",
    "    vert=False,\n",
    "    labels=np.array(mesures_train_X.columns[1:])[sorted_idx][0:20],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
