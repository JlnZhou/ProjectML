{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5b0b29-f218-42be-ad49-250258fa3f5c",
   "metadata": {},
   "source": [
    "On rajoute les données meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ade40bd-d76a-496b-a920-9cf77202eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46710d7f-b791-41d9-8e44-3e5b41f8d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_debit = pd.read_csv(\"../Data/Base/Stations_Debit.csv\")\n",
    "liste_stations_debit = list(stations_debit[\"Code station\"])\n",
    "\n",
    "mesures_train_X = pd.read_csv(\"../Data/Base/Mesures_Train_X.csv\")\n",
    "mesures_train_X[\"Date\"] = pd.to_datetime(mesures_train_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_train_Y = pd.read_csv(\"../Data/Base/Mesures_Train_Y.csv\")\n",
    "mesures_train_Y[\"Date\"] = pd.to_datetime(mesures_train_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "train_split_X = pd.read_csv(\"../Data/Base/Index_CV_X.csv\")\n",
    "train_split_Y = pd.read_csv(\"../Data/Base/Index_CV_Y.csv\")\n",
    "\n",
    "mesures_test_X = pd.read_csv(\"../Data/Base/Mesures_Test_X.csv\")\n",
    "mesures_test_X[\"Date\"] = pd.to_datetime(mesures_test_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_test_Y = pd.read_csv(\"../Data/Base/Mesures_Test_Y.csv\")\n",
    "mesures_test_Y[\"Date\"] = pd.to_datetime(mesures_test_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8752eee-0b85-4a1f-9781-d7b6d3d9e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo = pd.read_csv(\"../Data/Base/Stations_Meteo.csv\")\n",
    "liste_stations_meteo = list(stations_meteo[\"ID\"].apply(lambda x: str(x).rjust(5, \"0\")))\n",
    "colonnes_meteo = [\"Pression\", \"Vent_Nord\", \"Vent_Est\", \"Vitesse_vent\", \"Temperature\", \"Humidite\", \"Precipitations\"]\n",
    "tempo = []\n",
    "for code in liste_stations_meteo:\n",
    "    tempo += [tp + \"_\" + code for tp in colonnes_meteo]\n",
    "colonnes_meteo_stations = tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9732a271-7af8-4f7c-8840-2dcbbb4bb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Eval_fcts.py\n",
    "%run Standardize_fcts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c90c5f1-b157-46a8-b3cc-8034f8195fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Regression_GAM_Gamma.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b81e78-551c-47ca-9e0b-ebd3d94d52ea",
   "metadata": {},
   "source": [
    "# Val croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb59d54-eebd-4184-8065-200bd98d1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_knots = 10\n",
    "lr = 30\n",
    "max_lag = 7\n",
    "\n",
    "curr_alpha=3\n",
    "curr_compo=10\n",
    "curr_feature=\"log2\"\n",
    "curr_sample=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd14f45-0ebd-45df-83df-cd71c897e2a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▉                                       | 1/9 [06:47<54:19, 407.39s/it]\u001b[A\n",
      "  0%|                                                     | 0/1 [06:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10535/3171258819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m             reg2.fit(X = curr_train_X_standard_residus[colonnes_reg],\n\u001b[1;32m     84\u001b[0m               y = curr_train_X_standard_residus[code])\n\u001b[0;32m---> 85\u001b[0;31m             reg3.fit(X = curr_train_X_standard_residus[colonnes_reg],\n\u001b[0m\u001b[1;32m     86\u001b[0m               y = curr_train_X_standard_residus[code])\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    664\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for curr_lag in range(max_lag,max_lag+1):\n",
    "    cv_scores_RMSE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_MAE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_R2_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "    cv_scores_RMSE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_MAE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_R2 = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "    for curr_split in range(9):\n",
    "\n",
    "        # Donnees\n",
    "        curr_train_X = mesures_train_X[train_split_X[\"Train_\" + str(curr_split)]]\n",
    "        curr_train_Y = mesures_train_Y[train_split_Y[\"Train_\" + str(curr_split)]]\n",
    "        curr_test_X = mesures_train_X[train_split_X[\"Test_\" + str(curr_split)]]\n",
    "        curr_test_Y = mesures_train_Y[train_split_Y[\"Test_\" + str(curr_split)]]\n",
    "        liste_dates = curr_test_Y[\"Date\"]\n",
    "\n",
    "        curr_train_X_mean = curr_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "        for code in liste_stations_debit:\n",
    "            curr_train_X_mean[code] = 0\n",
    "        curr_train_X_std = curr_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "        curr_train_X_standard = fct_Standardize(curr_train_X, curr_train_X_mean,\n",
    "                                                curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "        curr_train_X_standard[\"Date\"] = curr_train_X[\"Date\"]\n",
    "        #GAM\n",
    "        spline_fit = fct_Regression_SplineGamma_fit(curr_train_X_standard, liste_stations_debit, n_knots)\n",
    "        curr_train_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit, \n",
    "                                                                           curr_train_X_standard, \n",
    "                                                                           liste_stations_debit)\n",
    "        curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, \n",
    "                                                      curr_train_X_standard[colonnes_meteo_stations]],\n",
    "                                                     axis = 1)\n",
    "\n",
    "         # Ajout AR\n",
    "        colonnes_reg = colonnes_meteo_stations\n",
    "        for i in tqdm(range(curr_lag+1)):\n",
    "            variable = curr_train_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "            variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "            curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, variable], axis = 1)\n",
    "            colonnes_reg = colonnes_reg + [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "        curr_train_X_standard_residus = curr_train_X_standard_residus[(7+curr_lag):]\n",
    "\n",
    "        curr_test_X_standard =fct_Standardize(curr_test_X, curr_train_X_mean,\n",
    "                                              curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "        curr_test_X_standard[\"Date\"] = curr_test_X[\"Date\"].values\n",
    "        curr_test_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit,\n",
    "                                                                          curr_test_X_standard, \n",
    "                                                                          liste_stations_debit)\n",
    "        curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, \n",
    "                                                  curr_test_X_standard[colonnes_meteo_stations]],\n",
    "                                                 axis = 1)\n",
    "        # Ajout variables AR\n",
    "        for i in range(curr_lag+1):\n",
    "            variable = curr_test_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "            variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "            curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, variable], axis = 1)\n",
    "        curr_test_X_standard_residus = curr_test_X_standard_residus[(7+curr_lag):]\n",
    "\n",
    "        curr_test_Y_standard = fct_Standardize(curr_test_Y, curr_train_X_mean,\n",
    "                                                curr_train_X_std, liste_stations_debit)\n",
    "        curr_test_Y_standard[\"Date\"] = curr_test_Y[\"Date\"]\n",
    "\n",
    "        curr_RMSE_standard = []\n",
    "        curr_MAE_standard = []\n",
    "        curr_R2_standard = []\n",
    "        curr_RMSE = []\n",
    "        curr_MAE = []\n",
    "        curr_R2 = []\n",
    "\n",
    "        for code in liste_stations_debit:\n",
    "\n",
    "            # Entrainement\n",
    "            reg1 = linear_model.Lasso(alpha = curr_alpha/1000, max_iter = 30000)\n",
    "            #reg2 = PLSRegression(n_components=curr_compo)\n",
    "            reg2 = RandomForestRegressor(max_features=curr_feature,\n",
    "                                            max_samples=curr_sample/10)\n",
    "            reg3 = GradientBoostingRegressor(learning_rate = lr/1000)\n",
    "            \n",
    "            reg1.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "              y = curr_train_X_standard_residus[code])\n",
    "            #reg2.fit(X = curr_train_X_standard_residus[colonnes_reg], Y = curr_train_X_standard_residus[code])\n",
    "            reg2.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "              y = curr_train_X_standard_residus[code])\n",
    "            reg3.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "              y = curr_train_X_standard_residus[code])\n",
    "            \n",
    "            model = VotingRegressor([(\"LASSO\", reg1), (\"PLS\", reg2), (\"RF\", reg3)])\n",
    "            #model = VotingRegressor([(\"LASSO\", reg1), (\"Random Forest\", reg2)])\n",
    "            model.fit(X= curr_train_X_standard_residus[colonnes_reg], y = curr_train_X_standard_residus[code])\n",
    "\n",
    "\n",
    "\n",
    "            # Predictions Standard\n",
    "            predictions_Y_standard_residus = model.predict(curr_test_X_standard_residus[colonnes_reg])\n",
    "            predictions_Y_standard_residus = pd.DataFrame(predictions_Y_standard_residus, columns=[code])\n",
    "            predictions_Y_standard_residus[\"Date\"] = curr_test_X_standard_residus[\"Date\"].values\n",
    "            # Filtre sur les dates\n",
    "            resultat = pd.DataFrame()\n",
    "            for curr_date in liste_dates:\n",
    "                resultat = pd.concat([resultat, predictions_Y_standard_residus[predictions_Y_standard_residus[\"Date\"] == curr_date]])\n",
    "            resultat = resultat.sort_values(by = \"Date\")\n",
    "            predictions_Y_standard_residus = resultat.copy()\n",
    "            # On rajoute la compo saisonnalité\n",
    "            predictions_Y_standard_saisonnalite = fct_Regression_SplineGamma_predict(spline_fit, liste_dates, [code])\n",
    "            predictions_Y_standard = predictions_Y_standard_residus[[\"Date\"]].copy()\n",
    "            predictions_Y_standard[code] = predictions_Y_standard_residus[code].values + predictions_Y_standard_saisonnalite[code].values\n",
    "\n",
    "            # Scores standards\n",
    "            curr_RMSE_standard.append(fct_RMSE(curr_test_Y_standard, predictions_Y_standard, [code])[\"RMSE\"][0])\n",
    "            curr_MAE_standard.append(fct_MAE(curr_test_Y_standard, predictions_Y_standard, [code])[\"MAE\"][0])\n",
    "            curr_R2_standard.append(fct_R2(curr_test_Y_standard, predictions_Y_standard, [code])[\"R2\"][0])\n",
    "            # Score\n",
    "            predictions_Y = fct_StandardizeInverse(predictions_Y_standard, \n",
    "                                                   curr_train_X_mean, curr_train_X_std, \n",
    "                                                   [code])\n",
    "            predictions_Y[\"Date\"] = predictions_Y_standard[\"Date\"].values\n",
    "            curr_RMSE.append(fct_RMSE(curr_test_Y, predictions_Y, [code])[\"RMSE\"][0])\n",
    "            curr_MAE.append(fct_MAE(curr_test_Y, predictions_Y, [code])[\"MAE\"][0])\n",
    "            curr_R2.append(fct_R2(curr_test_Y, predictions_Y, [code])[\"R2\"][0])\n",
    "\n",
    "        # On rassemble\n",
    "        cv_scores_RMSE_standard[\"Split_\" + str(curr_split)] = curr_RMSE_standard\n",
    "        cv_scores_MAE_standard[\"Split_\" + str(curr_split)] = curr_MAE_standard\n",
    "        cv_scores_R2_standard[\"Split_\" + str(curr_split)] = curr_R2_standard\n",
    "\n",
    "        cv_scores_RMSE[\"Split_\" + str(curr_split)] = curr_RMSE\n",
    "        cv_scores_MAE[\"Split_\" + str(curr_split)] = curr_MAE\n",
    "        cv_scores_R2[\"Split_\" + str(curr_split)] = curr_R2\n",
    "\n",
    "    # Calcul des scores moyens du split\n",
    "    cv_moyen_RMSE_standard = []\n",
    "    cv_moyen_MAE_standard = []\n",
    "    cv_moyen_R2_standard = []\n",
    "    cv_moyen_RMSE = []\n",
    "    cv_moyen_MAE = []\n",
    "    cv_moyen_R2 = []\n",
    "\n",
    "    for code in liste_stations_debit:\n",
    "        score_RMSE = np.mean(cv_scores_RMSE_standard[cv_scores_RMSE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_RMSE_standard.append(score_RMSE)\n",
    "        score_MAE = np.mean(cv_scores_MAE_standard[cv_scores_MAE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_MAE_standard.append(score_MAE)\n",
    "        score_R2 = np.mean(cv_scores_R2_standard[cv_scores_R2_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_R2_standard.append(score_R2)\n",
    "        score_RMSE = np.mean(cv_scores_RMSE[cv_scores_RMSE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_RMSE.append(score_RMSE)\n",
    "        score_MAE = np.mean(cv_scores_MAE[cv_scores_MAE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_MAE.append(score_MAE)\n",
    "        score_R2 = np.mean(cv_scores_R2[cv_scores_R2[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_R2.append(score_R2)\n",
    "\n",
    "    cv_scores_RMSE_standard[\"Moyenne\"] = cv_moyen_RMSE_standard\n",
    "    cv_scores_RMSE_standard.to_csv(\"../Data/GAMaggregAR/CV_RMSE_standard_\"+ \"_\" + str(curr_lag) + \".csv\",    \n",
    "                          index=False)\n",
    "    cv_scores_MAE_standard[\"Moyenne\"] = cv_moyen_MAE_standard\n",
    "    cv_scores_MAE_standard.to_csv(\"../Data/GAMaggregAR/CV_MAE_standard_\"  + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_R2_standard[\"Moyenne\"] = cv_moyen_R2_standard\n",
    "    cv_scores_R2_standard.to_csv(\"../Data/GAMaggregAR/CV_R2_standard_\"  + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_RMSE[\"Moyenne\"] = cv_moyen_RMSE\n",
    "    cv_scores_RMSE.to_csv(\"../Data/GAMaggregAR/CV_RMSE_\"  + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_MAE[\"Moyenne\"] = cv_moyen_MAE\n",
    "    cv_scores_MAE.to_csv(\"../Data/GAMaggregAR/CV_MAE_\"  + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_R2[\"Moyenne\"] = cv_moyen_R2\n",
    "    cv_scores_R2.to_csv(\"../Data/GAMaggregAR/CV_R2_\"  + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd468b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for curr_lag in tqdm(range(0, max_lag+1)):\n",
    "\n",
    "    cv_scores_RMSE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_MAE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_R2_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "    cv_scores_RMSE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_MAE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "    cv_scores_R2 = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "    for curr_split in range(9):\n",
    "        \n",
    "        # Entraintement\n",
    "        curr_train_X = mesures_train_X[train_split_X[\"Train_\" + str(curr_split)]]\n",
    "        curr_train_Y = mesures_train_Y[train_split_Y[\"Train_\" + str(curr_split)]]\n",
    "        curr_train_X_mean = curr_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "        for code in liste_stations_debit:\n",
    "            curr_train_X_mean[code] = 0\n",
    "        curr_train_X_std = curr_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "        curr_train_X_standard = fct_Standardize(curr_train_X, curr_train_X_mean,\n",
    "                                            curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "        curr_train_X_standard[\"Date\"] = curr_train_X[\"Date\"]\n",
    "            #GAM\n",
    "        spline_fit = fct_Regression_SplineGamma_fit(curr_train_X_standard, liste_stations_debit, n_knots)\n",
    "        curr_train_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit, \n",
    "                                                                           curr_train_X_standard, \n",
    "                                                                           liste_stations_debit)\n",
    "        curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, \n",
    "                                                  curr_train_X_standard[colonnes_meteo_stations]],\n",
    "                                                 axis = 1)\n",
    "            # Ajout AR\n",
    "        colonnes_reg = colonnes_meteo_stations\n",
    "        for i in range(curr_lag+1):\n",
    "            variable = curr_train_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "            variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "            curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, variable], axis = 1)\n",
    "            colonnes_reg = colonnes_reg + [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "        curr_train_X_standard_residus = curr_train_X_standard_residus[(7+curr_lag):]\n",
    "        \n",
    "        reg1 = linear_model.Lasso(alpha = curr_alpha/1000, max_iter = 30000)\n",
    "        #reg2 = PLSRegression(n_components=curr_compo)\n",
    "        reg2 = RandomForestRegressor(max_features=curr_feature,\n",
    "                                        max_samples=curr_sample/10)\n",
    "        #reg3 = GradientBoostingRegressor(learning_rate = lr)\n",
    "\n",
    "        reg1.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          y = curr_train_X_standard_residus[code])\n",
    "        #reg2.fit(X = curr_train_X_standard_residus[colonnes_reg], Y = curr_train_X_standard_residus[code])\n",
    "        reg2.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          y = curr_train_X_standard_residus[code])\n",
    "        #reg3.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          #y = curr_train_X_standard_residus[code])\n",
    "\n",
    "        #model = VotingRegressor([(\"LASSO\", reg1), (\"PLS\", reg2), (\"RF\", reg3)])\n",
    "        model = VotingRegressor([(\"LASSO\", reg1), (\"Random Forest\", reg2)])\n",
    "        model.fit(X= curr_train_X_standard_residus[colonnes_reg], y = curr_train_X_standard_residus[liste_stations_debit])\n",
    "\n",
    "        # Predictions Standard\n",
    "        curr_test_X = mesures_train_X[train_split_X[\"Test_\" + str(curr_split)]]\n",
    "        curr_test_Y = mesures_train_Y[train_split_Y[\"Test_\" + str(curr_split)]]\n",
    "        liste_dates = curr_test_Y[\"Date\"]\n",
    "        curr_test_X_standard =fct_Standardize(curr_test_X, curr_train_X_mean,\n",
    "                                          curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "        curr_test_X_standard[\"Date\"] = curr_test_X[\"Date\"].values\n",
    "        curr_test_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit,\n",
    "                                                                      curr_test_X_standard, \n",
    "                                                                      liste_stations_debit)\n",
    "        curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, \n",
    "                                                  curr_test_X_standard[colonnes_meteo_stations]],\n",
    "                                                 axis = 1)\n",
    "            # Ajout variables AR\n",
    "        for i in range(curr_lag+1):\n",
    "            variable = curr_test_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "            variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "            curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, variable], axis = 1)\n",
    "        curr_test_X_standard_residus = curr_test_X_standard_residus[(7+curr_lag):]\n",
    "            # RF\n",
    "        predictions_test_Y_standard_residus = model_RF.predict(curr_test_X_standard_residus[colonnes_reg])\n",
    "        predictions_test_Y_standard_residus = pd.DataFrame(predictions_test_Y_standard_residus, columns=liste_stations_debit)\n",
    "        predictions_test_Y_standard_residus[\"Date\"] = curr_test_X_standard_residus[\"Date\"].values\n",
    "        # Filtre sur les dates\n",
    "        resultat = pd.DataFrame()\n",
    "        for curr_date in liste_dates:\n",
    "            resultat = pd.concat([resultat, predictions_test_Y_standard_residus[predictions_test_Y_standard_residus[\"Date\"] == curr_date]])\n",
    "        resultat = resultat.sort_values(by = \"Date\")\n",
    "        predictions_test_Y_standard_residus = resultat.copy()\n",
    "        # On rajoute la compo saisonnalité\n",
    "        predictions_test_Y_standard_saisonnalite = fct_Regression_SplineGamma_predict(spline_fit, liste_dates, liste_stations_debit)\n",
    "        predictions_test_Y_standard = predictions_test_Y_standard_residus[[\"Date\"]].copy()\n",
    "        for code in liste_stations_debit:\n",
    "            predictions_test_Y_standard[code] = predictions_test_Y_standard_residus[code].values + predictions_test_Y_standard_saisonnalite[code].values\n",
    "\n",
    "\n",
    "        # Score standard\n",
    "        curr_test_Y_standard = fct_Standardize(curr_test_Y, \n",
    "                                               curr_train_X_mean, curr_train_X_std, \n",
    "                                               liste_stations_debit)\n",
    "        curr_test_Y_standard[\"Date\"] = curr_test_Y[\"Date\"]\n",
    "        curr_RMSE = fct_RMSE(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "        cv_scores_RMSE_standard[\"Split_\" + str(curr_split)] = curr_RMSE[\"RMSE\"]\n",
    "        curr_MAE = fct_MAE(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "        cv_scores_MAE_standard[\"Split_\" + str(curr_split)] = curr_MAE[\"MAE\"]\n",
    "        curr_R2 = fct_R2(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "        cv_scores_R2_standard[\"Split_\" + str(curr_split)] = curr_R2[\"R2\"]\n",
    "        # Score\n",
    "        predictions_test_Y = fct_StandardizeInverse(predictions_test_Y_standard, \n",
    "                                               curr_train_X_mean, curr_train_X_std, \n",
    "                                               liste_stations_debit)\n",
    "        predictions_test_Y[\"Date\"] = predictions_test_Y_standard[\"Date\"].values\n",
    "        curr_RMSE = fct_RMSE(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "        cv_scores_RMSE[\"Split_\" + str(curr_split)] = curr_RMSE[\"RMSE\"]\n",
    "        curr_MAE = fct_MAE(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "        cv_scores_MAE[\"Split_\" + str(curr_split)] = curr_MAE[\"MAE\"]\n",
    "        curr_R2 = fct_R2(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "        cv_scores_R2[\"Split_\" + str(curr_split)] = curr_R2[\"R2\"]\n",
    "\n",
    "    cv_moyen_RMSE_standard = []\n",
    "    cv_moyen_MAE_standard = []\n",
    "    cv_moyen_R2_standard = []\n",
    "\n",
    "    cv_moyen_RMSE = []\n",
    "    cv_moyen_MAE = []\n",
    "    cv_moyen_R2 = []\n",
    "\n",
    "    for code in liste_stations_debit:\n",
    "        score_RMSE = np.mean(cv_scores_RMSE_standard[cv_scores_RMSE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_RMSE_standard.append(score_RMSE)\n",
    "        score_MAE = np.mean(cv_scores_MAE_standard[cv_scores_MAE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_MAE_standard.append(score_MAE)\n",
    "        score_R2 = np.mean(cv_scores_R2_standard[cv_scores_R2_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_R2_standard.append(score_R2)\n",
    "        score_RMSE = np.mean(cv_scores_RMSE[cv_scores_RMSE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_RMSE.append(score_RMSE)\n",
    "        score_MAE = np.mean(cv_scores_MAE[cv_scores_MAE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_MAE.append(score_MAE)\n",
    "        score_R2 = np.mean(cv_scores_R2[cv_scores_R2[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "        cv_moyen_R2.append(score_R2)\n",
    "\n",
    "    cv_scores_RMSE_standard[\"Moyenne\"] = cv_moyen_RMSE_standard\n",
    "    cv_scores_RMSE_standard.to_csv(\"../Data/GAMRandomForestAR/CV_RMSE_standard_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_MAE_standard[\"Moyenne\"] = cv_moyen_MAE_standard\n",
    "    cv_scores_MAE_standard.to_csv(\"../Data/GAMRandomForestAR/CV_MAE_standard_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_R2_standard[\"Moyenne\"] = cv_moyen_R2_standard\n",
    "    cv_scores_R2_standard.to_csv(\"../Data/GAMRandomForestAR/CV_R2_standard_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_RMSE[\"Moyenne\"] = cv_moyen_RMSE\n",
    "    cv_scores_RMSE.to_csv(\"../Data/GAMRandomForestAR/CV_RMSE_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_MAE[\"Moyenne\"] = cv_moyen_MAE\n",
    "    cv_scores_MAE.to_csv(\"../Data/GAMRandomForestAR/CV_MAE_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "    cv_scores_R2[\"Moyenne\"] = cv_moyen_R2\n",
    "    cv_scores_R2.to_csv(\"../Data/GAMRandomForestAR/CV_R2_\" + str(curr_feature) + \"_\" + str(curr_sample) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                          index=False)\n",
    "print(\"OK\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f61ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d589c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_train_X_standard_residus[code].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366761a-87df-4e2e-8b01-12a21cbe4ae4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colonnes_meteo_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ad0e-3717-4b5f-b654-be16ba4dbd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819eda28-32e9-45df-96eb-3f544562eac8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cours_eau = 3\n",
    "cours_eau = list(np.unique(stations_debit[\"Cours eau\"]))\n",
    "cours_eau_cmap = cm.get_cmap(ListedColormap([\"red\", \"green\", \"blue\"]))\n",
    "cours_eau_couleur = pd.DataFrame({\"Cours eau\": cours_eau, \"Index\": range(n_cours_eau), \"Couleur\": [\"red\", \"green\", \"blue\"]})\n",
    "cours_eau_couleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f8fff-6421-4623-8bf1-33c74b8ca232",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_RMSE_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb63cdd-51fd-4507-bacf-9010be71a177",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_MAE_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342c8f-b28b-48ae-9f57-34d27b922168",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_R2_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f6b7b-6157-44ca-9696-fe3824c28e8b",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00efb65-f12b-4490-b3c5-cb53804ce78e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_RMSE_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    " + \".csv\")    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3f190-39d9-42a0-b1fb-eb56073dac81",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_MAE_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809a4cb-c188-490e-8dbb-d9962dfb95fb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lr in lr_grid:\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_R2_standard_lr_\" + str(curr_lr) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"lr\"] = curr_lr/1000\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "fig, axs = plt.subplots(n_cours_eau, 1, figsize = (15,5))\n",
    "for i in range(n_cours_eau):\n",
    "    stations = list(stations_debit[stations_debit[\"Cours eau\"] == list(cours_eau_couleur[cours_eau_couleur[\"Index\"] == i][\"Cours eau\"])[0]][\"Code station\"])\n",
    "    for code in stations:\n",
    "        axs[i].plot(cv_moyen[\"lr\"], cv_moyen[code],\n",
    "                   color = cours_eau_cmap(i))\n",
    "        axs[i].set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1ecee-9eb8-4953-ad7a-efbc07a9fd34",
   "metadata": {},
   "source": [
    "## Moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c518f-5a58-4ab0-bd86-ca4316c70175",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_RMSE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715d107-be51-467a-8278-4312869b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_RMSE_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c4cd7-7da7-4017-bd54-14680a31c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_RMSE_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d4432-845d-4d2f-b5db-8b5e1ec93b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_MAE_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e9a4d-90a9-4f3b-b462-e1ec594de7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_MAE_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988a053-0af1-4d04-9778-8a259f43b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_MAE_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f138cab-6305-4224-85cf-3900c2e1a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "\n",
    "for curr_lag in range(max_lag+1):\n",
    "    for curr_lr in lr_grid:\n",
    "        curr_moyen = {} \n",
    "        cv_scores = pd.read_csv(\"../Data/GAMaggregAR/CV_R2_standard_\" + str(curr_lr) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"lr\"] = curr_lr/1000\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88716ef-f4e0-4d7c-882f-419fe76067fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"lr\").mean().index, cv_moyen.groupby(\"lr\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_R2_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221a677-c8b9-43e3-b507-354e4cd63d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"Lags\").mean().index, cv_moyen.groupby(\"Lags\").mean()[liste_stations_debit].mean(axis = 1).values)\n",
    "plt.savefig(\"../Data/Figures/GAMaggregAR/GAMaggregAR_R2_AR.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb63b23-803c-4db2-b015-0e4ceac78a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMaggregAR/CV_RMSE_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540949d9-0aee-434c-939a-c4af87b87546",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMaggregAR/CV_MAE_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6351422-08c1-4124-ae54-245087e26077",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../Data/GAMaggregAR/CV_R2_standard_30_0.csv\")[\"Moyenne\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b546cd-b011-40e7-bfda-b739c6531a21",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4df88e-e968-418d-bdae-3bfc80075d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6925f2-3461-45c6-9fce-75678cc218d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donnees\n",
    "mesures_train_X_mean = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "mesures_train_X_std = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "mesures_train_X_standard = fct_Standardize(mesures_train_X, \n",
    "                                           mesures_train_X_mean, mesures_train_X_std, \n",
    "                                           liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_train_X_standard[\"Date\"] = mesures_train_X[\"Date\"]\n",
    "\n",
    "mesures_test_X_standard =fct_Standardize(mesures_test_X, mesures_train_X_mean, mesures_train_X_std, \n",
    "                                         liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_test_X_standard[\"Date\"] = mesures_test_X[\"Date\"].values\n",
    "mesures_test_Y_standard = fct_Standardize(mesures_test_Y, mesures_train_X_mean, mesures_train_X_std, \n",
    "                                          liste_stations_debit)\n",
    "mesures_test_Y_standard[\"Date\"] = mesures_test_Y[\"Date\"]\n",
    "liste_dates = mesures_test_Y[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3aa27-10de-4d3e-8de6-9a5985d20530",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_RMSE_standard = []\n",
    "scores_MAE_standard = []\n",
    "scores_R2_standard = []\n",
    "scores_RMSE = []\n",
    "scores_MAE = []\n",
    "scores_R2 = []\n",
    "\n",
    "predictions_test_Y_standard = mesures_test_Y[[\"Date\"]]\n",
    "predictions_test_Y = mesures_test_Y[[\"Date\"]]\n",
    "\n",
    "for code in tqdm(liste_stations_debit):\n",
    "        # Entrainement\n",
    "        # Entrainement\n",
    "        reg1 = linear_model.Lasso(alpha = curr_alpha/1000, max_iter = 30000)\n",
    "        #reg2 = PLSRegression(n_components=curr_compo)\n",
    "        reg2 = RandomForestRegressor(max_features=curr_feature,\n",
    "                                        max_samples=curr_sample/10)\n",
    "        #reg3 = GradientBoostingRegressor(learning_rate = lr)\n",
    "\n",
    "        reg1.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          y = curr_train_X_standard_residus[code])\n",
    "        #reg2.fit(X = curr_train_X_standard_residus[colonnes_reg], Y = curr_train_X_standard_residus[code])\n",
    "        reg2.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          y = curr_train_X_standard_residus[code])\n",
    "        #reg3.fit(X = curr_train_X_standard_residus[colonnes_reg],\n",
    "          #y = curr_train_X_standard_residus[code])\n",
    "\n",
    "        #model = VotingRegressor([(\"LASSO\", reg1), (\"PLS\", reg2), (\"RF\", reg3)])\n",
    "        model = VotingRegressor([(\"LASSO\", reg1), (\"Random Forest\", reg2)])\n",
    "        model.fit(X= curr_train_X_standard_residus[colonnes_reg], y = curr_train_X_standard_residus[code])\n",
    "\n",
    "        # Predictions Standard\n",
    "        predictions_Y_standard = model.predict(mesures_test_X_standard[colonnes_meteo_stations])\n",
    "        predictions_Y_standard = pd.DataFrame(predictions_Y_standard, columns=[code])\n",
    "        predictions_Y_standard[\"Date\"] = mesures_test_X_standard[\"Date\"].values\n",
    "        # Filtre sur les dates\n",
    "        resultat = pd.DataFrame()\n",
    "        for curr_date in liste_dates:\n",
    "            resultat = pd.concat([resultat, predictions_Y_standard[predictions_Y_standard[\"Date\"] == curr_date]])\n",
    "        resultat = resultat.sort_values(by = \"Date\")\n",
    "        predictions_Y_standard = resultat.copy()\n",
    "        predictions_test_Y_standard = predictions_test_Y_standard.merge(predictions_Y_standard)\n",
    "        \n",
    "        predictions_Y = fct_StandardizeInverse(predictions_Y_standard, \n",
    "                                                   mesures_train_X_mean, mesures_train_X_std, \n",
    "                                                   [code])\n",
    "        predictions_Y[\"Date\"] = predictions_Y_standard[\"Date\"].values\n",
    "        predictions_test_Y = predictions_test_Y.merge(predictions_Y)\n",
    "        \n",
    "        # Scores standards\n",
    "        scores_RMSE_standard.append(fct_RMSE(mesures_test_Y_standard, predictions_Y_standard, [code])[\"RMSE\"][0])\n",
    "        scores_MAE_standard.append(fct_MAE(mesures_test_Y_standard, predictions_Y_standard, [code])[\"MAE\"][0])\n",
    "        scores_R2_standard.append(fct_R2(mesures_test_Y_standard, predictions_Y_standard, [code])[\"R2\"][0])\n",
    "        # Score\n",
    "        scores_RMSE.append(fct_RMSE(mesures_test_Y, predictions_Y, [code])[\"RMSE\"][0])\n",
    "        scores_MAE.append(fct_MAE(mesures_test_Y, predictions_Y, [code])[\"MAE\"][0])\n",
    "        scores_R2.append(fct_R2(mesures_test_Y, predictions_Y, [code])[\"R2\"][0])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382f33f-4913-4ea7-bbd0-b4020f4f61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_RMSE,\n",
    "                            \"MAE\": scores_MAE,\n",
    "                            \"R2\": scores_R2})\n",
    "test_scores.to_csv(\"../Data/GAMaggregAR/Test_scores.csv\")\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e650086-8f4d-459b-9b3b-e17e68ed6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_standard = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_RMSE,\n",
    "                            \"MAE\": scores_MAE,\n",
    "                            \"R2\": scores_R2})\n",
    "test_scores_standard.to_csv(\"../Data/GAMaggregAR/Test_scores_standard.csv\")\n",
    "test_scores_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d21b-9d11-49d1-9b90-a14fc12e0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_standard[[\"RMSE\", \"MAE\", \"R2\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415553c-cdaf-4e30-84a3-e94c1b0ae4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = len(liste_stations_debit)\n",
    "fig, axs = plt.subplots(n_stations, 1, figsize = (15,30), sharex=True)\n",
    "for i in range(n_stations):\n",
    "    code = liste_stations_debit[i]\n",
    "    axs[i].plot(mesures_test_Y_standard[code], label = \"True\")\n",
    "    axs[i].plot(predictions_test_Y_standard[code], label = \"Predictions\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780d10b-c50d-4704-8573-8f505095a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = len(liste_stations_debit)\n",
    "fig, axs = plt.subplots(n_stations, 1, figsize = (15,30), sharex=True)\n",
    "for i in range(n_stations):\n",
    "    code = liste_stations_debit[i]\n",
    "    axs[i].plot(mesures_test_Y[code], label = \"True\")\n",
    "    axs[i].plot(predictions_test_Y[code], label = \"Predictions\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab74395-3ce5-42bd-bdae-87f1d887c33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
