{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5b0b29-f218-42be-ad49-250258fa3f5c",
   "metadata": {},
   "source": [
    "On rajoute les données meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ade40bd-d76a-496b-a920-9cf77202eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46710d7f-b791-41d9-8e44-3e5b41f8d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_debit = pd.read_csv(\"../Data/Base/Stations_Debit.csv\")\n",
    "liste_stations_debit = list(stations_debit[\"Code station\"])\n",
    "\n",
    "mesures_train_X = pd.read_csv(\"../Data/Base/Mesures_Train_X.csv\")\n",
    "mesures_train_X[\"Date\"] = pd.to_datetime(mesures_train_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_train_Y = pd.read_csv(\"../Data/Base/Mesures_Train_Y.csv\")\n",
    "mesures_train_Y[\"Date\"] = pd.to_datetime(mesures_train_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "train_split_X = pd.read_csv(\"../Data/Base/Index_CV_X.csv\")\n",
    "train_split_Y = pd.read_csv(\"../Data/Base/Index_CV_Y.csv\")\n",
    "\n",
    "mesures_test_X = pd.read_csv(\"../Data/Base/Mesures_Test_X.csv\")\n",
    "mesures_test_X[\"Date\"] = pd.to_datetime(mesures_test_X[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")\n",
    "mesures_test_Y = pd.read_csv(\"../Data/Base/Mesures_Test_Y.csv\")\n",
    "mesures_test_Y[\"Date\"] = pd.to_datetime(mesures_test_Y[\"Date\"], format = \"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8752eee-0b85-4a1f-9781-d7b6d3d9e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo = pd.read_csv(\"../Data/Base/Stations_Meteo.csv\")\n",
    "liste_stations_meteo = list(stations_meteo[\"ID\"].apply(lambda x: str(x).rjust(5, \"0\")))\n",
    "colonnes_meteo = [\"Pression\", \"Vent_Nord\", \"Vent_Est\", \"Vitesse_vent\", \"Temperature\", \"Humidite\", \"Precipitations\"]\n",
    "tempo = []\n",
    "for code in liste_stations_meteo:\n",
    "    tempo += [tp + \"_\" + code for tp in colonnes_meteo]\n",
    "colonnes_meteo_stations = tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9732a271-7af8-4f7c-8840-2dcbbb4bb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Eval_fcts.py\n",
    "%run Standardize_fcts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f8cbae-9cfd-430e-bce6-cd2b6b921579",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Regression_GAM_Gamma.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b81e78-551c-47ca-9e0b-ebd3d94d52ea",
   "metadata": {},
   "source": [
    "# Val croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd14f45-0ebd-45df-83df-cd71c897e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [365, 358]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3985/2019483175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             model_RF = RandomForestRegressor(max_features=feature,\n\u001b[1;32m     53\u001b[0m                                             max_samples=curr_sample/10)\n\u001b[0;32m---> 54\u001b[0;31m             model_RF.fit(X = curr_train_X_standard[colonnes_meteo_stations],\n\u001b[0m\u001b[1;32m     55\u001b[0m                   y = curr_train_X_standard_residus[liste_stations_debit])\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/geo/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [365, 358]"
     ]
    }
   ],
   "source": [
    "n_knots = 10\n",
    "max_compo = 100\n",
    "max_lag = 7\n",
    "depth = 5\n",
    "feature= \"log2\"\n",
    "sample_grid = [2, 4, 6, 8, 10]\n",
    "\n",
    "\n",
    "for curr_sample in tqdm(sample_grid):\n",
    "    for curr_lag in range(max_lag+1):\n",
    "        \n",
    "        cv_scores_RMSE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_MAE_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_R2_standard = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "\n",
    "        cv_scores_RMSE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_MAE = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        cv_scores_R2 = pd.DataFrame({\"Code station\": liste_stations_debit})\n",
    "        \n",
    "        for curr_split in range(9):\n",
    "\n",
    "            # Entrainnement\n",
    "            curr_train_X = mesures_train_X[train_split_X[\"Train_\" + str(curr_split)]]\n",
    "            curr_train_Y = mesures_train_Y[train_split_Y[\"Train_\" + str(curr_split)]]\n",
    "                #Standardisation Train_X\n",
    "            curr_train_X_mean = curr_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "            for code in liste_stations_debit:\n",
    "                curr_train_X_mean[code] = 0\n",
    "            curr_train_X_std = curr_train_X[liste_stations_debit].std()\n",
    "            curr_train_X_std = curr_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "            curr_train_X_standard = fct_Standardize(curr_train_X, curr_train_X_mean,\n",
    "                                                    curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "            curr_train_X_standard[\"Date\"] = curr_train_X[\"Date\"]\n",
    "                #GAM\n",
    "            spline_fit = fct_Regression_SplineGamma_fit(curr_train_X_standard, liste_stations_debit, n_knots)\n",
    "            curr_train_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit, \n",
    "                                                                                   curr_train_X_standard, \n",
    "                                                                                   liste_stations_debit)\n",
    "            curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, \n",
    "                                                      curr_train_X_standard[colonnes_meteo_stations]],\n",
    "                                                     axis = 1)\n",
    "            # Ajout AR\n",
    "            colonnes_reg = colonnes_meteo_stations\n",
    "            for i in range(curr_lag+1):\n",
    "                variable = curr_train_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "                variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "                curr_train_X_standard_residus = pd.concat([curr_train_X_standard_residus, variable], axis = 1)\n",
    "                colonnes_reg = colonnes_reg + [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "                \n",
    "            # RF\n",
    "            curr_train_X_standard_residus = curr_train_X_standard_residus[(7+curr_lag):]\n",
    "            model_RF = RandomForestRegressor(max_features=feature,\n",
    "                                            max_samples=curr_sample/10)\n",
    "            model_RF.fit(X = curr_train_X_standard[colonnes_meteo_stations],\n",
    "                  y = curr_train_X_standard_residus[liste_stations_debit])\n",
    "            \n",
    "            # Predictions GAM\n",
    "            curr_test_X = mesures_train_X[train_split_X[\"Test_\" + str(curr_split)]]\n",
    "            curr_test_Y = mesures_train_Y[train_split_Y[\"Test_\" + str(curr_split)]]\n",
    "            liste_dates = curr_test_Y[\"Date\"]\n",
    "                # Test_X standard\n",
    "            curr_test_X_standard =fct_Standardize(curr_test_X, curr_train_X_mean,\n",
    "                                                  curr_train_X_std, liste_stations_debit + colonnes_meteo_stations)\n",
    "            curr_test_X_standard[\"Date\"] = curr_test_X[\"Date\"].values\n",
    "            curr_test_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit,\n",
    "                                                                              curr_test_X_standard, \n",
    "                                                                              liste_stations_debit)\n",
    "            curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, \n",
    "                                                      curr_test_X_standard[colonnes_meteo_stations]],\n",
    "                                                     axis = 1)\n",
    "                # Ajout variables AR\n",
    "            for i in range(curr_lag+1):\n",
    "                variable = curr_test_X_standard_residus[liste_stations_debit].shift(i+7)\n",
    "                variable.columns = [code+\"_\"+str(i) for code in liste_stations_debit]\n",
    "                curr_test_X_standard_residus = pd.concat([curr_test_X_standard_residus, variable], axis = 1)\n",
    "\n",
    "                # Pred RF\n",
    "            curr_test_X_standard_residus = curr_test_X_standard_residus[(7+curr_lag):]\n",
    "            predictions_test_Y_standard_residus = model_RF.predict(curr_test_X_standard_residus[colonnes_reg])\n",
    "            predictions_test_Y_standard_residus = pd.DataFrame(predictions_test_Y_standard_residus, columns=liste_stations_debit)\n",
    "            predictions_test_Y_standard_residus[\"Date\"] = curr_test_X_standard_residus[\"Date\"].values\n",
    "\n",
    "            # Filtre sur les dates\n",
    "            resultat = pd.DataFrame()\n",
    "            for curr_date in liste_dates:\n",
    "                resultat = pd.concat([resultat, predictions_test_Y_standard_residus[predictions_test_Y_standard_residus[\"Date\"] == curr_date]])\n",
    "            resultat = resultat.sort_values(by = \"Date\")\n",
    "            predictions_test_Y_standard_residus = resultat.copy()\n",
    "\n",
    "            # On rajoute la compo saisonnalité\n",
    "            predictions_test_Y_standard_saisonnalite = fct_Regression_SplineGamma_predict(spline_fit, liste_dates, liste_stations_debit)\n",
    "            predictions_test_Y_standard = predictions_test_Y_standard_residus[[\"Date\"]].copy()\n",
    "            for code in liste_stations_debit:\n",
    "                predictions_test_Y_standard[code] = predictions_test_Y_standard_residus[code].values + predictions_test_Y_standard_saisonnalite[code].values\n",
    "\n",
    "            # Score standard\n",
    "            curr_test_Y_standard = fct_Standardize(curr_test_Y, \n",
    "                                                   curr_train_X_mean, curr_train_X_std, \n",
    "                                                   liste_stations_debit)\n",
    "            curr_test_Y_standard[\"Date\"] = curr_test_Y[\"Date\"]\n",
    "            curr_RMSE = fct_RMSE(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "            cv_scores_RMSE_standard[\"Split_\" + str(curr_split)] = curr_RMSE[\"RMSE\"]\n",
    "            curr_MAE = fct_MAE(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "            cv_scores_MAE_standard[\"Split_\" + str(curr_split)] = curr_MAE[\"MAE\"]\n",
    "            curr_R2 = fct_R2(curr_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "            cv_scores_R2_standard[\"Split_\" + str(curr_split)] = curr_R2[\"R2\"]\n",
    "\n",
    "            # Score\n",
    "            predictions_test_Y = fct_StandardizeInverse(predictions_test_Y_standard, \n",
    "                                                   curr_train_X_mean, curr_train_X_std, liste_stations_debit)\n",
    "            predictions_test_Y[\"Date\"] = predictions_test_Y_standard[\"Date\"].values\n",
    "            curr_RMSE = fct_RMSE(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "            cv_scores_RMSE[\"Split_\" + str(curr_split)] = curr_RMSE[\"RMSE\"]\n",
    "            curr_MAE = fct_MAE(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "            cv_scores_MAE[\"Split_\" + str(curr_split)] = curr_MAE[\"MAE\"]\n",
    "            curr_R2 = fct_R2(curr_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "            cv_scores_R2[\"Split_\" + str(curr_split)] = curr_R2[\"R2\"]\n",
    "\n",
    "        cv_moyen_RMSE_standard = []\n",
    "        cv_moyen_MAE_standard = []\n",
    "        cv_moyen_R2_standard = []\n",
    "\n",
    "        cv_moyen_RMSE = []\n",
    "        cv_moyen_MAE = []\n",
    "        cv_moyen_R2 = []\n",
    "\n",
    "        for code in liste_stations_debit:\n",
    "            score_RMSE = np.mean(cv_scores_RMSE_standard[cv_scores_RMSE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_RMSE_standard.append(score_RMSE)\n",
    "            score_MAE = np.mean(cv_scores_MAE_standard[cv_scores_MAE_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_MAE_standard.append(score_MAE)\n",
    "            score_R2 = np.mean(cv_scores_R2_standard[cv_scores_R2_standard[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_R2_standard.append(score_R2)\n",
    "            score_RMSE = np.mean(cv_scores_RMSE[cv_scores_RMSE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_RMSE.append(score_RMSE)\n",
    "            score_MAE = np.mean(cv_scores_MAE[cv_scores_MAE[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_MAE.append(score_MAE)\n",
    "            score_R2 = np.mean(cv_scores_R2[cv_scores_R2[\"Code station\"] == code][[\"Split_\" + str(i) for i in range(9)]].iloc[0,:])\n",
    "            cv_moyen_R2.append(score_R2)\n",
    "\n",
    "        cv_scores_RMSE_standard[\"Moyenne\"] = cv_moyen_RMSE_standard\n",
    "        cv_scores_RMSE_standard.to_csv(\"../Data/GAMRFAR/CV_RMSE_standard_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_MAE_standard[\"Moyenne\"] = cv_moyen_MAE_standard\n",
    "        cv_scores_MAE_standard.to_csv(\"../Data/GAMRFAR/CV_MAE_standard_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_R2_standard[\"Moyenne\"] = cv_moyen_R2_standard\n",
    "        cv_scores_R2_standard.to_csv(\"../Data/GAMRFAR/CV_R2_standard_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_RMSE[\"Moyenne\"] = cv_moyen_RMSE\n",
    "        cv_scores_RMSE.to_csv(\"../Data/GAMRFAR/CV_RMSE_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_MAE[\"Moyenne\"] = cv_moyen_MAE\n",
    "        cv_scores_MAE.to_csv(\"../Data/GAMRFAR/CV_MAE_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)\n",
    "        cv_scores_R2[\"Moyenne\"] = cv_moyen_R2\n",
    "        cv_scores_R2.to_csv(\"../Data/GAMRFAR/CV_R2_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\",\n",
    "                              index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3900fe36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 273)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_train_X_standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa17e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_train_X_standard_residus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ad0e-3717-4b5f-b654-be16ba4dbd9c",
   "metadata": {},
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1ecee-9eb8-4953-ad7a-efbc07a9fd34",
   "metadata": {},
   "source": [
    "## Moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c518f-5a58-4ab0-bd86-ca4316c70175",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "for curr_compo in range(1, max_compo+1):\n",
    "    for curr_lag in range(max_lag+1):\n",
    "        curr_moyen = {}\n",
    "        cv_scores = pd.read_csv(\"../Data/GAMRFAR/CV_RMSE_standard_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"N_Compo\"] = curr_compo\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c697edb-92e7-4173-9d77-1a2e006e2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen[\"N_Compo\"], cv_moyen[liste_stations_debit].mean(axis = 1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ca894-a6fc-4a6a-8b0c-cb0c6339cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"N_Compo\").mean().index, cv_moyen.groupby(\"N_Compo\").mean()[liste_stations_debit].mean(axis = 1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d4432-845d-4d2f-b5db-8b5e1ec93b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "for curr_compo in range(1, max_compo+1):\n",
    "    for curr_lag in range(max_lag+1):\n",
    "        curr_moyen = {}\n",
    "        cv_scores = pd.read_csv(\"../Data/GAMRFAR/CV_R2_standard_\" + str(curr_compo) + \"_\" + str(curr_lag) + \".csv\")\n",
    "        for code in liste_stations_debit:\n",
    "            curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "        curr_moyen = pd.DataFrame(curr_moyen)\n",
    "        curr_moyen[\"N_Compo\"] = curr_compo\n",
    "        curr_moyen[\"Lags\"] = curr_lag\n",
    "        cv_moyen = pd.concat([cv_moyen, curr_moyen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989d04b-6e15-41bf-821b-86fb4eeb17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cv_moyen.groupby(\"N_Compo\").mean().index, cv_moyen.groupby(\"N_Compo\").mean()[liste_stations_debit].mean(axis = 1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f138cab-6305-4224-85cf-3900c2e1a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen = pd.DataFrame()\n",
    "for curr_compo in range(1, max_compo+1):\n",
    "    curr_moyen = {}\n",
    "    cv_scores = pd.read_csv(\"../Data/GAMRF_Meteo/CV_R2_standard_compo_\" + str(curr_compo) + \".csv\")\n",
    "    for code in liste_stations_debit:\n",
    "        curr_moyen[code] = list(cv_scores[cv_scores[\"Code station\"] == code][\"Moyenne\"])\n",
    "    curr_moyen = pd.DataFrame(curr_moyen)\n",
    "    curr_moyen[\"N_Compo\"] = curr_compo\n",
    "    cv_moyen = pd.concat([cv_moyen, curr_moyen])\n",
    "\n",
    "plt.plot(cv_moyen[\"N_Compo\"], cv_moyen[liste_stations_debit].mean(axis = 1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ecb37-298d-42a2-ab80-c34daba6c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_moyen[liste_stations_debit].mean(axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b546cd-b011-40e7-bfda-b739c6531a21",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4df88e-e968-418d-bdae-3bfc80075d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_compo = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415553c-cdaf-4e30-84a3-e94c1b0ae4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraintement\n",
    "mesures_train_X_mean = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].mean()\n",
    "for code in liste_stations_debit:\n",
    "    mesures_train_X_mean[code] = 0\n",
    "mesures_train_X_std = mesures_train_X[liste_stations_debit + colonnes_meteo_stations].std()\n",
    "mesures_train_X_standard = fct_Standardize(mesures_train_X, \n",
    "                                           mesures_train_X_mean, mesures_train_X_std, \n",
    "                                           liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_train_X_standard[\"Date\"] = mesures_train_X[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb503c-ecbb-4838-984a-149020ba42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_fit = fct_Regression_SplineGamma_fit(mesures_train_X_standard, liste_stations_debit, n_knots)\n",
    "mesures_train_X_standard_residus = fct_Regression_SplineGamma_residus(spline_fit, \n",
    "                                                                      mesures_train_X_standard, \n",
    "                                                                               liste_stations_debit)\n",
    "model_PLS = PLSRegression(n_components=n_compo)\n",
    "model_PLS.fit(X = mesures_train_X_standard[colonnes_meteo_stations],\n",
    "              Y = mesures_train_X_standard_residus[liste_stations_debit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b35f7-0ef8-455e-8b3e-9a953a64de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dates = mesures_test_Y[\"Date\"]\n",
    "            # Test_X standard\n",
    "mesures_test_X_standard =fct_Standardize(mesures_test_X, \n",
    "                                         mesures_train_X_mean, mesures_train_X_std, \n",
    "                                         liste_stations_debit + colonnes_meteo_stations)\n",
    "mesures_test_X_standard[\"Date\"] = mesures_test_X[\"Date\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e240f9-d9ac-46ff-ac1c-129330edf174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred\n",
    "predictions_test_Y_standard_residus = model_PLS.predict(mesures_test_X_standard[colonnes_meteo_stations])\n",
    "predictions_test_Y_standard_residus = pd.DataFrame(predictions_test_Y_standard_residus, columns=liste_stations_debit)\n",
    "predictions_test_Y_standard_residus[\"Date\"] = mesures_test_X_standard[\"Date\"].values\n",
    "# Filtre sur les dates\n",
    "resultat = pd.DataFrame()\n",
    "for curr_date in liste_dates:\n",
    "    resultat = pd.concat([resultat, predictions_test_Y_standard_residus[predictions_test_Y_standard_residus[\"Date\"] == curr_date]])\n",
    "resultat = resultat.sort_values(by = \"Date\")\n",
    "predictions_test_Y_standard_residus = resultat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b7583-4e21-4cd7-b0e4-b4eb8b0ebfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On rajoute la compo saisonnalité\n",
    "predictions_test_Y_standard_saisonnalite = fct_Regression_SplineGamma_predict(spline_fit, liste_dates, liste_stations_debit)\n",
    "predictions_test_Y_standard = predictions_test_Y_standard_residus[[\"Date\"]].copy()\n",
    "for code in liste_stations_debit:\n",
    "    predictions_test_Y_standard[code] = predictions_test_Y_standard_residus[code].values + predictions_test_Y_standard_saisonnalite[code].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08f556-0fc1-4fd4-9c4f-9f5422bfe9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesures_test_Y_standard = fct_Standardize(mesures_test_Y,\n",
    "                                          mesures_train_X_mean, mesures_train_X_std,\n",
    "                                          liste_stations_debit)\n",
    "mesures_test_Y_standard[\"Date\"] = mesures_test_Y[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419c3e1-306f-4101-a756-263ccba9bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = len(liste_stations_debit)\n",
    "fig, axs = plt.subplots(n_stations, 1, figsize = (15,30), sharex=True)\n",
    "for i in range(n_stations):\n",
    "    code = liste_stations_debit[i]\n",
    "    axs[i].plot(mesures_test_Y_standard[\"Date\"], mesures_test_Y_standard[code], label = \"True\")\n",
    "    #axs[i].plot(predictions_test_Y_standard_saisonnalite[\"Date\"], predictions_test_Y_standard_saisonnalite[code], label = \"Saisonnalite\")\n",
    "    #axs[i].plot(predictions_test_Y_standard_residus[\"Date\"], predictions_test_Y_standard_residus[code], label = \"Residus\")\n",
    "    axs[i].plot(predictions_test_Y_standard[\"Date\"], predictions_test_Y_standard[code], label = \"Predictions\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e755d89-4f21-4e12-91ff-d7726a5c6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rmse_standard = fct_RMSE(mesures_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "scores_mae_standard = fct_MAE(mesures_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "scores_r2_standard = fct_R2(mesures_test_Y_standard, predictions_test_Y_standard, liste_stations_debit)\n",
    "\n",
    "test_scores_standard = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_rmse_standard[\"RMSE\"],\n",
    "                            \"MAE\": scores_mae_standard[\"MAE\"],\n",
    "                            \"R2\": scores_r2_standard[\"R2\"]})\n",
    "test_scores_standard.to_csv(\"../Data/GAMRFAR/Test_scores_standard.csv\",\n",
    "                   index = False)\n",
    "test_scores_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cce98-8f28-4ab6-ab85-164ebc3374f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_standard[[\"RMSE\", \"MAE\", \"R2\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf98b8c-4286-4f8d-afab-9c91f910358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_Y = fct_StandardizeInverse(predictions_test_Y_standard, \n",
    "                                            mesures_train_X_mean, mesures_train_X_std, liste_stations_debit)\n",
    "predictions_test_Y[\"Date\"] = predictions_test_Y_standard[\"Date\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bff839-ff29-4050-97f4-fa3ee923c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rmse = fct_RMSE(mesures_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "scores_mae = fct_MAE(mesures_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "scores_r2 = fct_R2(mesures_test_Y, predictions_test_Y, liste_stations_debit)\n",
    "\n",
    "test_scores = pd.DataFrame({\"Code station\": liste_stations_debit,\n",
    "                            \"RMSE\": scores_rmse[\"RMSE\"],\n",
    "                            \"MAE\": scores_mae[\"MAE\"],\n",
    "                            \"R2\": scores_r2[\"R2\"]})\n",
    "test_scores.to_csv(\"../Data/GAMRFAR/Test_scores.csv\",\n",
    "                   index = False)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab74395-3ce5-42bd-bdae-87f1d887c33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
